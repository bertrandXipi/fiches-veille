---
title: "We reduced Claude API costs by 94.5% using a file tiering system (with proof) : r/ClaudeAI"
source_url: "https://www.reddit.com/r/ClaudeAI/s/Yd05o1Sj4J"
source_type: article
date_captured: "2026-01-28T18:21:36.841Z"
date_processed: "2026-01-28T18:22:41.208Z"
tags: []
language: fr
ingest_source: discord
discord_message_url: "https://discord.com/channels/1026842752232734811/1449479522993836213/1466136146303123560"
status: published
notebooklm_notebook_id: c4dba600-dd91-4027-ba33-8ad93f971a31
notebooklm_source_id: 1d228dea-8320-4a4d-8a65-f8baab89511c
notebooklm_url: "https://notebooklm.google.com/notebook/c4dba600-dd91-4027-ba33-8ad93f971a31"
keywords:
  - API cost reduction
  - File tiering system
  - Token optimization
  - Claude context management
  - Documentation efficiency
---

## R√©sum√© (NotebookLM)

Voici un rapport d√©taill√© analysant la discussion et le projet pr√©sent√©s sur le subreddit r/ClaudeAI concernant l'optimisation des co√ªts de l'API Claude.

# Rapport d‚ÄôAnalyse : Optimisation des Co√ªts API Claude via la Hi√©rarchisation des Fichiers

### 1. Le Contexte et les Id√©es Principales

Le fil de discussion principal porte sur une probl√©matique centrale pour les d√©veloppeurs utilisant des LLM (Large Language Models) via API : le co√ªt √©lev√© li√© √† la consommation de "tokens" lors de l'envoi de contextes volumineux [1].

L'auteur du post (jantonca) pr√©sente une solution open-source nomm√©e **cortex-tms**, con√ßue pour r√©duire drastiquement ces co√ªts. L'id√©e fondatrice est que la majorit√© des fichiers d'un projet (documentation archiv√©e, changelogs, vieux sprints) sont rarement n√©cessaires pour une t√¢che donn√©e, mais que Claude les "lit" et les facture quand m√™me s'ils sont inclus dans la fen√™tre contextuelle [1, 2].

Le concept cl√© propos√© est un **syst√®me de hi√©rarchisation des fichiers (File Tiering System)** inspir√© de la gestion des donn√©es (Hot/Warm/Cold storage). L'objectif est de ne charger par d√©faut que ce qui est strictement n√©cessaire, traitant la base de code comme un "bureau" (fichiers actifs) plut√¥t qu'une "armoire √† archives" [2, 3].

### 2. Les Diff√©rents Points de Vue et Arguments

L'analyse des √©changes r√©v√®le plusieurs courants de pens√©e :

*   **L'Approche "Ing√©nierie du Contexte" (L'Auteur) :** L'auteur d√©fend une approche pragmatique o√π l'on garde tout l'historique dans le d√©p√¥t (y compris les r√©trospectives et d√©cisions de design), mais o√π l'on utilise un outil pour masquer dynamiquement ce contenu au LLM. Il argue que cela permet de conserver le contexte historique sans polluer le contexte actif ni gonfler la facture [3].
*   **La Critique de l'Hygi√®ne du Code (Sceptiques) :** Un contre-argument fort est port√© par l'utilisateur *unwitty*, qui sugg√®re que les √©conomies annonc√©es (94,5 %) sont artificielles. Selon lui, le d√©p√¥t de l'auteur est rempli de "d√©chets" inutiles (vieux sprints, archives). Il compare cette m√©thode √† "commander six desserts, en manger un seul, et pr√©tendre avoir r√©duit son apport calorique de 94,5 %" [4]. Pour ces critiques, la solution est de nettoyer le d√©p√¥t, pas de cr√©er un filtre complexe.
*   **L'Approche "Documentation Vivante" (Alternative) :** D'autres utilisateurs, comme *RumLovingPirate*, proposent une philosophie diff√©rente bas√©e sur le "Journaling". Au lieu de trier des fichiers, ils demandent √† l'IA de maintenir elle-m√™me des documents de synth√®se (roadmaps, journaux de bord) de moins de 200 lignes. L'IA s'appuie alors sur ces r√©sum√©s plut√¥t que sur le code brut pour comprendre le contexte [5, 6].

### 3. D√©tails Techniques, Exemples Concrets et Donn√©es

Le rapport fournit des donn√©es pr√©cises issues du "dogfooding" (test interne) de l'outil sur le projet *cortex-tms* :

*   **Le Syst√®me de Tiers :**
    *   **HOT (Chaud) :** T√¢ches actives et travail en cours (environ 3 647 tokens). Charg√© par d√©faut [2].
    *   **WARM (Ti√®de) :** Mod√®les (patterns), glossaire, docs r√©centes (environ 10 419 tokens). Charg√© si n√©cessaire [2].
    *   **COLD (Froid) :** Archives, vieux sprints, changelogs (environ 52 768 tokens). Presque jamais charg√© [2].
*   **R√©sultats Financiers et Techniques :**
    *   **R√©duction de tokens :** Passage de 66 834 tokens (sans tri) √† 3 647 tokens (avec tri), soit une baisse de **94,5 %** [7].
    *   **Co√ªt par session :** Pour le mod√®le Claude Sonnet 4.5, le co√ªt passe de 1,20 $ (√©quivalent GPT-4 sans tri) ou 0,11 $ (prix optimis√©) √† environ 0,01 $ par session [7]. L'auteur pr√©cise que son calcul de 0,11 $ correspond √† une requ√™te unique en entr√©e (input tokens) [8].
*   **Impl√©mentation :** L'outil utilise des balises manuelles dans le code, par exemple `<!-- @cortex-tms-tier HOT -->`, et une interface en ligne de commande (CLI) pour valider les tiers [7].

### 4. Probl√®mes, D√©fis et Limitations Identifi√©s

Plusieurs limitations ont √©t√© soulev√©es lors de la discussion :

*   **La Gestion Manuelle :** La n√©cessit√© de "taguer" manuellement chaque fichier ou section est vue comme une friction. Un utilisateur demande : "Dois-je taguer les fichiers et mettre √† jour les tags manuellement ?" [9].
*   **Risque de Confidentialit√© et Pertinence :** L'inclusion de "r√©trospectives de sprint" (souvent √©motionnelles et candides) dans le codebase, m√™me en cat√©gorie COLD, est jug√©e risqu√©e et √©trange par certains d√©veloppeurs, qui estiment que ces sentiments humains ne devraient pas √™tre immortalis√©s dans le code [4].
*   **Complexit√© vs Nettoyage :** Comme mentionn√© plus haut, le syst√®me pallie potentiellement une mauvaise gestion des archives plut√¥t que de r√©soudre le probl√®me √† la source (supprimer les fichiers obsol√®tes) [10].

### 5. Solutions, Recommandations et Perspectives

Pour r√©pondre aux limitations, plusieurs pistes d'am√©lioration et alternatives ont √©t√© propos√©es :

*   **Automatisation via Git :** L'id√©e la plus populaire pour le futur est d'utiliser l'historique Git pour d√©terminer automatiquement la "chaleur" d'un fichier. Les fichiers modifi√©s r√©cemment seraient automatiquement "HOT", √©vitant le taggage manuel [11, 12].
*   **Architecture RAG (Retrieval-Augmented Generation) :** L'auteur et d'autres utilisateurs envisagent de faire √©voluer le syst√®me vers une architecture RAG ou l'utilisation de serveurs MCP (Model Context Protocol). Cela permettrait de stocker les donn√©es WARM/COLD dans une base vectorielle et de ne r√©cup√©rer que les fragments pertinents, d√©passant ainsi la simple s√©lection de fichiers [13, 14].
*   **Alternatives Structurelles :**
    *   Utiliser des fichiers de configuration JSON ou `.gitattributes` √† la racine pour g√©rer les tiers au lieu de polluer le code avec des balises [10, 15].
    *   Utiliser des noms de fichiers s√©mantiques et demander √† Claude de lister les r√©pertoires (`ls`) pour d√©couvrir seul le contexte pertinent [11].

### 6. Synth√®se Critique et Implications Pratiques

Cette √©tude de cas illustre une transition importante dans le d√©veloppement assist√© par IA. Nous passons d'une √®re o√π l'on "bourrait" le contexte (profitant des fen√™tres de 200k+ tokens) √† une √®re d'**optimisation contextuelle**.

**Implications Pratiques :**
1.  **√âconomie :** Pour les entreprises utilisant l'API √† grande √©chelle, la r√©duction de 90 % du contexte inutile est imp√©rative, non seulement pour le co√ªt direct, mais aussi pour la latence et l'empreinte carbone [16].
2.  **Qualit√© :** R√©duire le bruit (fichiers obsol√®tes) am√©liore la qualit√© des r√©ponses de l'IA, car elle se concentre sur la documentation √† jour [16].
3.  **Cache :** Ce syst√®me optimise l'utilisation du "Prompt Caching" d'Anthropic. En gardant le tier WARM stable et le tier HOT petit, on maximise les chances de r√©utiliser le cache (cache hit), r√©duisant encore les co√ªts [17].

**Conclusion Critique :**
Bien que l'outil *cortex-tms* soit une solution ing√©nieuse, la critique sur l'hygi√®ne du d√©p√¥t reste valide. L'outil est particuli√®rement utile pour les projets "legacy" ou les documentations lourdes, mais ne doit pas servir d'excuse pour conserver des archives inutiles dans un d√©p√¥t de code actif. L'avenir de cette m√©thode r√©side probablement dans son automatisation (via Git) et son int√©gration avec des protocoles comme MCP, rendant le tri transparent pour le d√©veloppeur.

## Mots-cl√©s

- **API cost reduction**
- **File tiering system**
- **Token optimization**
- **Claude context management**
- **Documentation efficiency**

## üìö NotebookLM

[Ouvrir dans NotebookLM](https://notebooklm.google.com/notebook/c4dba600-dd91-4027-ba33-8ad93f971a31)

Utilisez NotebookLM pour:
- Poser des questions approfondies sur le contenu
- G√©n√©rer des r√©sum√©s personnalis√©s selon vos besoins
- Cr√©er des podcasts audio pour √©couter en d√©placement
- Explorer les concepts et leurs interconnexions
- Comparer avec d'autres sources du notebook

## Source

- [Article original](https://www.reddit.com/r/ClaudeAI/s/Yd05o1Sj4J)
