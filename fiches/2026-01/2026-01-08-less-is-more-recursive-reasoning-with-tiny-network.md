---
title: "Less is More: Recursive Reasoning with Tiny Networks"
source_url: "https://arxiv.org/pdf/2510.04871"
source_type: article
date_captured: "2026-01-08T08:06:11.223Z"
date_processed: "2026-01-08T13:00:35.602Z"
category: IA
tags:
  - LLM
  - SLM
  - ARC-AGI
  - Récursion
  - Efficacité
reading_time_minutes: null
language: fr
llm_provider: gemini
llm_model: gemini-cli
llm_prompt_version: v1
ingest_source: discord
discord_message_url: "https://discord.com/channels/1026842752232734811/1449479522993836213/1458733511463800924"
status: published
---

## Résumé

Cette étude présente le Tiny Recursive Model (TRM), une architecture de réseau de neurones minimaliste (7 millions de paramètres) capable de surpasser les modèles de langage géants (LLMs) sur des tâches de raisonnement logique complexes. En utilisant une approche de raisonnement récursif où le modèle boucle sur lui-même pour affiner ses réponses, le TRM démontre que l'intelligence peut émerger de structures compactes sans nécessiter une mise à l'échelle massive des paramètres.

Le document souligne que le TRM atteint des performances supérieures à des modèles comme Deepseek R1 ou Gemini sur les puzzles ARC-AGI, tout en utilisant moins de 0,01 % de leurs paramètres. Cette découverte remet en question la domination du paradigme 'plus c'est gros, mieux c'est' et ouvre la voie à une IA spécialisée, économe en énergie et capable de fonctionner localement sur des appareils mobiles.

## Points clés

- Le modèle TRM de 7M de paramètres surpasse des modèles comme o3-mini et Gemini sur le benchmark ARC-AGI.
- L'utilisation de la récursion permet de simuler un processus de réflexion itératif avec un coût computationnel réduit.
- Démontre que l'architecture récursive est plus efficace pour la généralisation logique que la simple augmentation de la taille des modèles.

## Concepts liés

[[Tiny Recursive Model]] | [[Recursive Reasoning]] | [[ARC-AGI Benchmark]] | [[Inference Scaling]] | [[Model Compactness]]

## Note originale

Aucune note.

## Source

- [Article original](https://arxiv.org/pdf/2510.04871)
